\chapter{Kết luận và hướng phát triển}
\label{Chapter5}

\section{Kết luận}

Trong khóa luận này, chúng tôi nghiên cứu về bài toán huấn luyện mạng nơ-ron nhiều tầng ẩn bằng thuật toán Adam. Cụ thể, chúng tôi tập trung tìm hiểu các khó khăn của việc huấn luyện một mạng nơ-ron nhiều tầng ẩn, cũng như cách mà các thuật toán tối ưu tiếp cận và giải quyết các khó khăn đó, với trọng tâm được đặt vào thuật toán Adam \cite{kingma2014adam}. Dưới đây là một số kết quả đạt được của khóa luận.

Chúng tôi tìm hiểu, cài đặt lại và sử dụng các thuật toán SGD, Momentum, Adagrad, RMSprop, Adam (cùng một số thuật toán khác) để kiểm tra cách hoạt động của các thuật toán này trong nhiều tình huống khác nhau, từ mô phỏng bằng dữ liệu ngẫu nhiên đến huấn luyện những kiến trúc mạng nơ-ron được sử dụng trong thực tế trên các tập dữ liệu phổ biến. Ngoài ra, chúng tôi cũng ứng dụng các phương pháp tăng tốc tính toán như véc-tơ hóa trên CPU (Central Processing Unit) bằng thư viện Numpy và tính toán song song trên GPU (Graphics Processing Unit) bằng thư viện Pytorch. Kết quả của các thuật toán do chúng tôi cài đặt có thể tái tạo được một phần kết quả được công bố trong bài báo. Ngoài ra, với kiến trúc VGG16 và tập dữ liệu ImageNet, chúng tôi sử dụng TPU (Tensor Processing Unit) trên nền tảng Google Colaboratory để rút ngắn thời gian thực hiện thí nghiệm.

Chúng tôi thực hiện một số thí nghiệm nhằm phân tích khả năng tối ưu của các thuật toán trong nhiều tình huống khác nhau. Chẳng hạn, chúng tôi phân tích cách mà các thuật toán di chuyển trong bề mặt rãnh hẹp, cụ thể là độ lớn của các bước cập nhật theo từng trục của rãnh. Ngoài ra, chúng tôi cũng xem xét các trường hợp tối ưu và không tối ưu của Adam. Kết quả cho thấy rằng Adam không chỉ phát huy được ưu điểm của các thuật toán cơ sở là Momentum và RMSprop về tốc độ và khả năng thích ứng tỉ lệ học, mà còn khắc phục được một số điểm yếu của các thuật toán này như sự dao động và khả năng sửa sai khi di chuyển lố qua khỏi điểm cực tiểu. Chúng tôi cũng thực hiện thí nghiệm trên các kiến trúc mạng nơ-ron nhiều tầng ẩn được sử dụng phổ biến hiện nay, và cho thấy rằng Adam thường đạt được kết quả tốt nhất.

Chúng tôi cũng phân tích một số hạn chế của thuật toán như:

\begin{itemize}
	\item Thời gian cần cho từng bước tối ưu của Adam lâu hơn các thuật toán còn lại do Adam thực hiện tính toán cả véc-tơ quán tính lẫn thích ứng tỉ lệ học. Ngoài ra, Adam còn có thêm công đoạn bias-correction để các bước tối ưu đầu tiên không bị bias về 0. Tuy nhiên, sau khi đã thực hiện được nhiều bước cập nhật thì bước bias-correction này không còn cần thiết nữa. Vì vậy, về lâu dài, Adam vẫn tốn thời gian cho bước bias-correction nhưng lại không giúp ích cho quá trình cập nhật.
	\item Trong trường hợp rãnh hẹp có các trục không trùng với trọng số, Adam cũng gặp khó khăn như các thuật toán tỉ lệ học thích ứng khác. Các thuật toán này thực hiện thay đổi tỉ lệ học cho từng trọng số một cách riêng biệt, dẫn đến hiệu quả bị giảm sút khi rãnh hẹp có trục chéo, tức là có sự phụ thuộc giữa các trọng số với nhau.
\end{itemize}

\section{Hướng phát triển}

Thuật toán Adam là thuật toán huấn luyện mạng nơ-ron nhiều tầng ẩn đầu tiên kết hợp cả hai phương pháp cải thiện quá trình tối ưu trước đó là tăng tốc bằng quán tính và thích ứng tỉ lệ học cho từng trọng số trong mô hình. Kể từ khi được công bố tại hội nghị International Conference on Learning Representations 2015, Adam đã trở thành thuật toán tối ưu mặc định cho bài toán huấn luyện mạng nơ-ron nhiều tầng ẩn, với khoảng 76 nghìn lượt trích dẫn. Khóa luận này chỉ mới cho thấy được phần nào ý tưởng của thuật toán khi kết hợp các phương pháp cải thiện, cũng như cho thấy Adam đạt được kết quả rất tốt trong nhiều tình huống khác nhau. Định hướng phát triển của khóa luận là tìm hiểu thêm về những nguyên lý để giải thích cho sự hiệu quả của Adam, cụ thể là:

\begin{itemize}
	\item Với mỗi tình huống gặp phải trong quá trình di chuyển trên bề mặt lỗi, các biến trung bình chạy của Adam sẽ có sự biến động tương ứng. Khác với các thuật toán Momentum và RMSprop, hai biến trung bình chạy của Adam cùng ảnh hưởng đến lượng cập nhật cho bộ trọng số. Vì vậy, chúng tôi sẽ tìm hiểu hiểu thêm về cách mỗi biến trung bình chạy ảnh hưởng lên bước cập nhật. Ngoài ra, liệu sự biến đổi của một biến trung bình chạy có hỗ trợ khắc phục cho các vấn đề của biến còn lại trong các trường hợp khó tối ưu hay không cũng là một vấn đề mà chúng tôi sẽ nghiên cứu.
	\item Việc Adam có sử dụng nguyên lý của Momentum trong quá trình cập nhật giúp thuật toán xấp xỉ true gradient thông qua gradient của các minibatch tốt hơn. Chúng tôi sẽ nghiên cứu xem liệu chúng ta có thể sử dụng kích thước minibatch nhỏ hơn mà vẫn giữ được sử ổn định hay không. Nếu chúng ta có thể sử dụng kích thước mininbatch nhỏ hơn mà không phải hi sinh sự ổn định, chúng ta sẽ giảm đáng kể thời gian cần thiết để huấn luyện các mạng nơ-ron nhiều tầng ẩn.
	\item Các thí nghiệm của chúng tôi chỉ thể hiện một lần chạy cho mỗi thuật toán, vì vậy các kết quả thu được vẫn còn yếu tố ngẫu nhiên. Để có thể thu thập được kết quả thể hiện tốt hơn khả năng huấn luyện mạng nơ-ron nhiều tầng ẩn của các thuật toán, chúng tôi sẽ thực hiện các thí nghiệm nhiều lần hơn. Việc có thêm nhiều điểm dữ liệu cho cùng một cài đặt thí nghiệm sẽ thể hiện được chính xác xu hướng chung của mỗi thuật toán hơn, đồng thời phát hiện được các trường hợp ngoại lệ.
	\item Cuối cùng, từ khi Adam được giới thiệu vào năm 2015, đã có nhiều bài báo cải tiến từ cơ sở của Adam ở các khía cạnh khác nhau. Một số ví dụ có thể kể đến là Nadam \cite{dozat2016incorporating} thay thế quán tính của Adam bằng Nesterov, AdamW \cite{loshchilov2018decoupled} thay đổi nguyên lý suy biến trọng số (weight decay), và AdaBelief \cite{zhuang2020adabelief} sử dụng đồng thời véc-tơ gradient và véc-tơ quán tính cho việc thích ứng tỉ lệ học. Từ nền tảng của thuật toán Adam, chúng tôi sẽ tìm hiểu thêm về những cải tiến này và hiệu quả của chúng trong các bài toán khác nhau.
\end{itemize}